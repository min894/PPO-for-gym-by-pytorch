# PPO-for-gym-by-pytorch
使用ppo (clip)算法实现gym和lrschool里的一些任务
# 改进
1.用蒙特卡罗法计算目标回报值时舍弃之前的用下一状态的价值预估，而是之间将采样的轨迹reward值累加。因为采样的轨迹是有序的（不像DQN这些是无序的所以不行）所以可以累加。  

2.在loss函数加入agent输出动作分布的熵，可能会加速收敛。让agent做出更加笃定的行为。  

3.固定一次训练的批次（固定一次性agent看到的“视野”），不管这个episode是否结束，如果结束则继续采样下一次episode知道采样的数达到预定值。效率会提升。  

# 使用
环境：python pytorch gym lrschool  

开始使用前必须先设置参数，主要看任务的要求，有些是离散动作空间有些是连续的，还要设置动作空间的范围。  

# 联系方式  

vx: 18357687883  
欢迎交流^_^
